<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      publications | Ran Zhu (朱然)
    
  
</title>
<meta name="author" content="Ran Zhu">
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">



<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->




  <link rel="shortcut icon" href="/assets/img/tud.svg?d0ba7b8be8a678824f294af8669286e4">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/publications/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>










  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          Ran Zhu (朱然)
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  <a class="nav-link" href="/publications/">publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/awards/">awards
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/services/">services
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                
                <li class="nav-item ">
                  <a class="nav-link" href="/contact/">contact
                    
                  </a>
                </li>
              
            
          
            
          
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        

<div class="post">
  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <!-- _pages/publications.md -->

<!-- Bibsearch Feature -->

<div class="publications">

<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">IMWUT’24</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/shufflefl.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="shufflefl.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="zhu2024shufflefl" class="col-sm-8">
    <!-- Title -->
    <div class="title">ShuffleFL: Addressing heterogeneity in multi-device federated learning</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Ran
            Zhu</em>, Mingkun
            Yang, and Qing
            Wang
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</em>,  2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://dl-acm-org.tudelft.idm.oclc.org/doi/abs/10.1145/3659621" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Federated Learning (FL) has emerged as a privacy-preserving paradigm for collaborative deep learning model training across distributed data silos. Despite its importance, FL faces challenges such as high latency and less effective global models. In this paper, we propose ShuffleFL, an innovative framework stemming from the hierarchical FL, which introduces a user layer between the FL devices and the FL server. ShuffleFL naturally groups devices based on their affiliations, e.g., belonging to the same user, to ease the strict privacy restriction-"data at the FL devices cannot be shared with others", thereby enabling the exchange of local samples among them. The user layer assumes a multi-faceted role, not just aggregating local updates but also coordinating data shuffling within affiliated devices. We formulate this data shuffling as an optimization problem, detailing our objectives to align local data closely with device computing capabilities and to ensure a more balanced data distribution at the intra-user devices. Through extensive experiments using realistic device profiles and five non-IID datasets, we demonstrate that ShuffleFL can improve inference accuracy by 2.81% to 7.85% and speed up the convergence by 4.11x to 36.56x when reaching the target accuracy.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">ICLR’24</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/fedtrans.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fedtrans.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="yang2024fedtrans" class="col-sm-8">
    <!-- Title -->
    <div class="title">FedTrans: Client-Transparent Utility Estimation for Robust Federated Learning</div>
    <!-- Author -->
    <div class="author">
      

      
      Mingkun
            Yang<sup>*</sup>, <em>Ran
            Zhu<sup>*</sup></em>, Qing
            Wang, and Jie
            Yang
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* Equal Contribution">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Twelfth International Conference on Learning Representations (ICLR)</em>,  2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://openreview.net/forum?id=DRu8PMHgCh" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
        <a href="https://github.com/Ran-ZHU/FedTrans" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Federated Learning (FL) is an important privacy-preserving learning paradigm that plays an important role in the Intelligent Internet of Things. Training a global model in FL, however, is vulnerable to the noise in the heterogeneous data across the clients. In this paper, we introduce **FedTrans**, a novel client-transparent client utility estimation method designed to guide client selection for noisy scenarios, mitigating performance degradation problems. To estimate the client utility, we propose a Bayesian framework that models client utility and its relationships with the weight parameters and the performance of local models. We then introduce a variational inference algorithm to effectively infer client utility, given only a small amount of auxiliary data. Our evaluation demonstrates that leveraging FedTrans as a guide for client selection can lead to a better accuracy performance (up to 7.8%), ensuring robustness in noisy scenarios.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">Communications Mag.</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/magzine.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="magzine.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="10341545" class="col-sm-8">
    <!-- Title -->
    <div class="title">Centimeter-Level Indoor Visible Light Positioning</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Ran
            Zhu</em>, Maxim
            Abeele, Jona
            Beysens, Jie
            Yang, and Qing
            Wang
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>IEEE Communications Magazine</em>,  2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://ieeexplore-ieee-org.tudelft.idm.oclc.org/abstract/document/10341545" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Visible light positioning (VLP) based on the received signal strength (RSS) can leverage a dense deployment of LEDs in future lighting infrastructure to provide accurate and energy-efficient indoor positioning. However, its positioning accuracy heavily depends on the density of collected fingerprints, which is labor-intensive. In this work, we propose a data pre-processing method, including data cleaning and data augmentation, to construct reliable and dense fingerprint samples, thereby alleviating the impact of noisy samples as well as reducing labor intensity. Extensive experiments demonstrate that our proposed method achieves an average positioning error of 1.7 cm, utilizing a sparse dataset that reduces the fingerprint collection effort by 98 percent. Running a tinyML-based model for VLP on the Arduino Nano microcontroller, we also show the possibilities for deploying RSS fingerprint-based VLP systems on resource-constrained embedded devices for real-world applications.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2023</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">SECON’23</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/fednawi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fednawi.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="10287422" class="col-sm-8">
    <!-- Title -->
    <div class="title">FedNaWi: Selecting the Befitting Clients for Robust Federated Learning in IoT Applications</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Ran
            Zhu</em>, Mingkun
            Yang, Jie
            Yang, and Qing
            Wang
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 20th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)</em>,  2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://ieeexplore-ieee-org.tudelft.idm.oclc.org/abstract/document/10287422" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Federated Learning (FL) is an important privacy-preserving learning paradigm that is expected to play an essential role in the future Intelligent Internet of Things (IoT). However, model training in FL is vulnerable to noise and the statistical heterogeneity of local data across IoT clients. In this paper, we propose FedNaWi, a “Go Narrow, Then Wide” client selection method that speeds up the FL training, achieves higher model performance, while requiring no additional data or sensitive information transfer from clients. Our method first selects reliable clients (i.e., going narrow) which allows the global model to quickly improve its performance and then includes less reliable clients (i.e., going wide) to exploit more IoT data of clients to further improve the global model. To profile client utility, we introduce a unified Bayesian framework to model the client utility at the FL server, assisted by a small amount of auxiliary data. We conduct extensive evaluations with 5 state-of-the-art FL methods, on 3 IoT tasks and under 7 different types of label and feature noise. We build an FL testbed with 38 IoT nodes (20 nodes run on Raspberry Pi 4B and 18 nodes run on Jetson Nano) for the evaluation. Our results show that FedNaWi improves the FL accuracy substantially and significantly reduces energy consumption. In particular, FedNaWi improves the accuracy from 35% to 75% in the non-IID Dirichlet setting, and reduces the average energy consumption by 55%.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">Neurocomputing</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/contextavo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="contextavo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="song2023contextavo" class="col-sm-8">
    <!-- Title -->
    <div class="title">ContextAVO: Local context guided and refining poses for deep visual odometry</div>
    <!-- Author -->
    <div class="author">
      

      
      Rujun
            Song, <em>Ran
            Zhu</em>, Zhuoling
            Xiao, and Bo
            Yan
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Neurocomputing</em>,  2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0925231223001534" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Learning-based monocular visual odometry (VO) has lately drawn significant attention for its robustness to camera parameters and environmental variations. The correlation of ego-motion in the local time dimension, denoted as the local context, is crucial for alleviating accumulated errors of VO problems. Unlike most current learning-based methods, our approach, called ContextAVO, focuses on the effectiveness of local contexts to improve the estimation recovered from consecutive multiple optical flow snippets. To retain the pose consistency in the temporal domain, we design the Context-Attention Refining component to adaptively ameliorate current inference by exploiting the continuity of camera motions and aligning corresponding observations with local contexts. Besides, we employ the multi-length window to make ContextAVO more suitable for general scenarios and less dependent on the fixed length of the input snippet. Extensive experiments on outdoor KITTI, Malaga, ApolloScape, and indoor TUM RGB-D datasets have demonstrated that our approach efficiently produces competitive results against classic algorithms. It outperforms state-of-the-art methods by large margins, improving up to 7.40% and 48.56% for translational and rotational estimation, respectively.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">Neurocomputing</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/deepavo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="deepavo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="zhu2022deepavo" class="col-sm-8">
    <!-- Title -->
    <div class="title">ContextAVO: Local context guided and refining poses for deep visual odometry</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Ran
            Zhu</em>, Mingkun
            Yang, Wang
            Liu, Rujun
            Song, Bo
            Yan, and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'Zhuoling Xiao' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Neurocomputing</em>,  2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://www-sciencedirect-com.tudelft.idm.oclc.org/science/article/pii/S0925231221013874" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The technology for Visual Odometry (VO) that estimates the position and orientation of the moving object through analyzing the image sequences captured by on-board cameras, has been well investigated with the rising interest in autonomous driving. This paper studies monocular VO from the perspective of Deep Learning (DL). Unlike most current learning-based methods, our approach, called DeepAVO, is established on the intuition that features contribute discriminately to different motion patterns. Specifically, we present a novel four-branch network to learn the rotation and translation by leveraging Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input. To enhance the ability of feature selection, we further introduce an effective channel-spatial attention mechanism to force each branch to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets involving outdoor driving and indoor walking scenarios show that the proposed DeepAVO outperforms the state-of-the-art monocular methods by a large margin, demonstrating competitive performance to the stereo VO algorithm and verifying promising potential for generalization.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">Sensors Journal</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/imu.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="imu.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="9471853" class="col-sm-8">
    <!-- Title -->
    <div class="title">Symmetrical-Net: Adaptive zero velocity detection for ZUPT-aided pedestrian navigation system</div>
    <!-- Author -->
    <div class="author">
      

      
      Mingkun
            Yang, <em>Ran
            Zhu</em>, Zhuoling
            Xiao, and Bo
            Yan
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>IEEE Sensors Journal</em>,  2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://ieeexplore-ieee-org.tudelft.idm.oclc.org/abstract/document/9471853" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Inertial navigation system (INS) is a practical method for indoor pedestrian navigation without pre-installation of infrastructure. Based on the fundamentals of human bipedal motion, zero velocity update (ZUPT) is a pervasive approach to tackle the accumulated error of inertial measurement units (IMU). While zero velocity detection plays a vital role in the algorithm, existing fixed-threshold methods to pick these pseudo-measurements of error-state Kalman Filter (ESKF) have the doubtful capability to fit various individuals in different motions. To address this issue, we propose the Symmetrical-Net leveraging deep Recurrent Convolutional Neural Networks (RCNNs) to detect the zero velocity interval adaptively. Additionally, two RCNNs are constructed in the symmetrical framework considering bidirectional IMU readings, which further improves the performance of the model. A comprehensive evaluation containing 87 different trajectories from 27 individuals has been conducted. The results show that the detection accuracy is up to 99.5% and 96.5% on the training and validation sets, respectively. It is verified that the precise and robust Symmetrical-Net can be a viable approach for the ZUPT-aided INS system.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">IEEE Access</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
    
    <img src="/assets/img/publication_preview/har_journal.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="har_journal.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="8734079" class="col-sm-8">
    <!-- Title -->
    <div class="title">Efficient Human Activity Recognition Solving the Confusing Activities Via Deep Ensemble Learning</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Ran
            Zhu</em>, Zhuoling
            Xiao, Ying
            Li, Mingkun
            Yang, Yawen
            Tan, and
        <span class="more-authors" title="click to view 3 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '3 more authors' ? 'Liang Zhou, Shuisheng Lin, Hongkai Wen' : '3 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.html(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">3 more authors</span>
      
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>IEEE Access</em>,  2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
        
          <a href="https://ieeexplore-ieee-org.tudelft.idm.oclc.org/abstract/document/8734079" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a>
        
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The ubiquity of smartphones and their rich set of on-board sensors has created many exciting new opportunities, where smartphones are used as powerful computing platforms to sense and analyze pervasive data. One important application of mobile sensing is activity recognition based on smartphone inertial sensors, which is a fundamental building block for a variety of scenarios, such as indoor pedestrian tracking, mobile health care, and smart cities. Although many approaches have been proposed to address the human activity recognition problem, several challenges are still present: 1) people’s motion modes are very different for different individuals; 2) there is only a very limited amount of training data; 3) human activities can be arbitrary and complex, thus handcrafted feature engineering often fails to work; and 4) the recognition accuracy tends to be limited due to confusing activities. To tackle those challenges, in this paper, we propose a human activity recognition framework based on convolutional neural networks (CNNs) with two convolutional layers using the smartphone-based accelerometer, gyroscope, and magnetometer. To solve the confusion between highly similar activities like going upstairs and walking, this paper presents a novel ensemble model of CNN to further improve the identification accuracy. The extensive experiments have been conducted using 235 977 sensory samples from a total of 100 subjects. The results have shown that the classification accuracy of the proposed model can be up to 96.11%, which proves the effectiveness of the proposed model.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>

</div>

  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    


  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      
  © Copyright 2025
  Ran
  
  Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

  
  
    Last updated: March 19, 2025.
  

    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>


  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script>





















  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>






<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

<!-- Badges -->

  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>



  <!-- MathJax -->
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
  
    <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  









  <!-- Scrolling Progress Bar -->
  <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script>







  <!-- Back to Top -->
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>






  </body>
</html>
